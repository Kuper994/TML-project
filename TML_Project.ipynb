{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "HxIlYSguDxRf",
        "C0vyp0-jyMAo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a62f7314ec6046719cbfdc40e7e2dfa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9566ccf60a5245b49656796c5cfefa5b",
              "IPY_MODEL_0b786494b1eb47d588b4e5319223e061",
              "IPY_MODEL_9fadee7435be413ea1f6fd69298c9dfb"
            ],
            "layout": "IPY_MODEL_758be631d4e24e6faee3aea4ffeef944"
          }
        },
        "9566ccf60a5245b49656796c5cfefa5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e7d2cb1b88841d5a80cdd88f92ef1b6",
            "placeholder": "​",
            "style": "IPY_MODEL_aa4e4809192548ffb709fcd9209ed3e7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0b786494b1eb47d588b4e5319223e061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52fb9484fa7646f8bfa5fa8bd3243935",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2274f5dd3dd94fb79fab0e6afe2044a0",
            "value": 2
          }
        },
        "9fadee7435be413ea1f6fd69298c9dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f7d369d1b944cb58146fbf00bb3ccc9",
            "placeholder": "​",
            "style": "IPY_MODEL_f2fe978002d34984ab62c49da50eb83d",
            "value": " 2/2 [01:24&lt;00:00, 38.57s/it]"
          }
        },
        "758be631d4e24e6faee3aea4ffeef944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7d2cb1b88841d5a80cdd88f92ef1b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa4e4809192548ffb709fcd9209ed3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52fb9484fa7646f8bfa5fa8bd3243935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2274f5dd3dd94fb79fab0e6afe2044a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f7d369d1b944cb58146fbf00bb3ccc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2fe978002d34984ab62c49da50eb83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kuper994/TML-project/blob/main/TML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Env Setup"
      ],
      "metadata": {
        "id": "HxIlYSguDxRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U torchtext==0.18.0\n"
      ],
      "metadata": {
        "id": "ehs7kWV09WHZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKRE8NkYDgYi",
        "outputId": "d355f97c-9e59-4896-f31a-4cfe8e4d5d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Trutworthy-ML/Project')\n",
        "sys.path.append(os.path.abspath('/content/drive/MyDrive/Trutworthy-ML/Project'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BartForSequenceClassification\n",
        "import transformers\n",
        "import random\n",
        "import pickle\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "RNRJp8otZkvq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model llama"
      ],
      "metadata": {
        "id": "zBqMr_RfFDbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.pipelines.text_generation import ReturnType, Chat\n",
        "from typing import Union, Sequence, Any\n",
        "\n",
        "\n",
        "class OurPipeline(transformers.pipelines.TextGenerationPipeline):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "  def _forward(self, model_inputs, **generate_kwargs):\n",
        "        input_ids = model_inputs[\"input_ids\"]\n",
        "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
        "        # Allow empty prompts\n",
        "        if input_ids.shape[1] == 0:\n",
        "            input_ids = None\n",
        "            attention_mask = None\n",
        "            in_b = 1\n",
        "        else:\n",
        "            in_b = input_ids.shape[0]\n",
        "        prompt_text = model_inputs.pop(\"prompt_text\")\n",
        "\n",
        "        # If there is a prefix, we may need to adjust the generation length. Do so without permanently modifying\n",
        "        # generate_kwargs, as some of the parameterization may come from the initialization of the pipeline.\n",
        "        prefix_length = generate_kwargs.pop(\"prefix_length\", 0)\n",
        "        if prefix_length > 0:\n",
        "            has_max_new_tokens = \"max_new_tokens\" in generate_kwargs or (\n",
        "                \"generation_config\" in generate_kwargs\n",
        "                and generate_kwargs[\"generation_config\"].max_new_tokens is not None\n",
        "            )\n",
        "            if not has_max_new_tokens:\n",
        "                generate_kwargs[\"max_length\"] = generate_kwargs.get(\"max_length\") or self.model.config.max_length\n",
        "                generate_kwargs[\"max_length\"] += prefix_length\n",
        "            has_min_new_tokens = \"min_new_tokens\" in generate_kwargs or (\n",
        "                \"generation_config\" in generate_kwargs\n",
        "                and generate_kwargs[\"generation_config\"].min_new_tokens is not None\n",
        "            )\n",
        "            if not has_min_new_tokens and \"min_length\" in generate_kwargs:\n",
        "                generate_kwargs[\"min_length\"] += prefix_length\n",
        "\n",
        "        # BS x SL\n",
        "        generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, output_scores=True, return_dict_in_generate=True, **generate_kwargs)\n",
        "        # print(generated_sequence)\n",
        "        # forward_res = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # out_b = generated_sequence.shape[0]\n",
        "        # if self.framework == \"pt\":\n",
        "        #     generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
        "        # elif self.framework == \"tf\":\n",
        "        #     pass\n",
        "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"prompt_text\": prompt_text}  #, \"forward_res\": forward_res}\n",
        "\n",
        "  def forward(self, model_inputs, **forward_params):\n",
        "        with self.device_placement():\n",
        "            if self.framework == \"tf\":\n",
        "                model_inputs[\"training\"] = False\n",
        "                model_outputs = self._forward(model_inputs, **forward_params)\n",
        "            elif self.framework == \"pt\":\n",
        "                inference_context = self.get_inference_context()\n",
        "                with inference_context():\n",
        "                    model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)\n",
        "                    model_outputs = self._forward(model_inputs, **forward_params)\n",
        "                    model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(\"cpu\"))\n",
        "            else:\n",
        "                raise ValueError(f\"Framework {self.framework} is not supported\")\n",
        "        return model_outputs\n",
        "\n",
        "  def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, clean_up_tokenization_spaces=True):\n",
        "        generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
        "        input_ids = model_outputs[\"input_ids\"]\n",
        "        prompt_text = model_outputs[\"prompt_text\"]\n",
        "        generated_sequence = generated_sequence.numpy().tolist()\n",
        "        records = []\n",
        "        for sequence in generated_sequence:\n",
        "            if return_type == ReturnType.TENSORS:\n",
        "                record = {\"generated_token_ids\": sequence}\n",
        "            elif return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:\n",
        "                # Decode text\n",
        "                text = self.tokenizer.decode(\n",
        "                    sequence,\n",
        "                    skip_special_tokens=True,\n",
        "                    clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
        "                )\n",
        "\n",
        "                # Remove PADDING prompt of the sequence if XLNet or Transfo-XL model is used\n",
        "                if input_ids is None:\n",
        "                    prompt_length = 0\n",
        "                else:\n",
        "                    prompt_length = len(\n",
        "                        self.tokenizer.decode(\n",
        "                            input_ids[0],\n",
        "                            skip_special_tokens=True,\n",
        "                            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                all_text = text[prompt_length:]\n",
        "                if return_type == ReturnType.FULL_TEXT:\n",
        "                    if isinstance(prompt_text, str):\n",
        "                        all_text = prompt_text + all_text\n",
        "                    elif isinstance(prompt_text, Chat):\n",
        "                        # Explicit list parsing is necessary for parsing chat datasets\n",
        "                        all_text = list(prompt_text.messages) + [{\"role\": \"assistant\", \"content\": all_text}]\n",
        "\n",
        "                record = {\"generated_text\": all_text}\n",
        "            records.append(record)\n",
        "        return model_outputs, records\n",
        "\n"
      ],
      "metadata": {
        "id": "Ajg2LrJ57l3q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"The recent advances in computational biology are\"\n",
        "access_token = \"\"\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# # model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    token=access_token\n",
        " )\n",
        "\n"
      ],
      "metadata": {
        "id": "qgdtdBRBIUJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a62f7314ec6046719cbfdc40e7e2dfa6",
            "9566ccf60a5245b49656796c5cfefa5b",
            "0b786494b1eb47d588b4e5319223e061",
            "9fadee7435be413ea1f6fd69298c9dfb",
            "758be631d4e24e6faee3aea4ffeef944",
            "0e7d2cb1b88841d5a80cdd88f92ef1b6",
            "aa4e4809192548ffb709fcd9209ed3e7",
            "52fb9484fa7646f8bfa5fa8bd3243935",
            "2274f5dd3dd94fb79fab0e6afe2044a0",
            "1f7d369d1b944cb58146fbf00bb3ccc9",
            "f2fe978002d34984ab62c49da50eb83d"
          ]
        },
        "outputId": "11448d67-0221-435e-8f1e-367741c3225e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a62f7314ec6046719cbfdc40e7e2dfa6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generator = transformers.pipeline(\n",
        "\"text-generation\",\n",
        "model=model,\n",
        "tokenizer=tokenizer,\n",
        "torch_dtype=torch.float16,\n",
        "device=device,\n",
        "# pipeline_class=OurPipeline,\n",
        ")\n",
        "target = tokenizer.encode(\"NLP models have been increasingly applied in the field of biology\")\n",
        "\n",
        "class_model = BartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\", num_labels = 2, ignore_mismatched_sizes = True)\n",
        "classifier = transformers.pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=class_model,\n",
        "    device=device)"
      ],
      "metadata": {
        "id": "NjB7O4SlA_48"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness(sequence, candidate_labels, classifier, generator, tokenizer, min_value=-10e2, max_length=300):\n",
        "    sequences = generator(\n",
        "                          sequence,\n",
        "                          do_sample=True,\n",
        "                          top_k=10,\n",
        "                          num_return_sequences=1,\n",
        "                          eos_token_id=tokenizer.eos_token_id,\n",
        "                          max_length=max_length,\n",
        "                        )\n",
        "    text = sequences[0]['generated_text']\n",
        "    results = classifier(text, candidate_labels)\n",
        "    fit_score = dict(zip(results['labels'], results['scores']))['computational biology']\n",
        "\n",
        "    return fit_score\n",
        "\n",
        "def create_initial_population(vocabulary, prompt_len, population_size: int = 200):\n",
        "    population = []\n",
        "    for _ in range(population_size - 1):\n",
        "        population.append(random.sample(vocabulary, prompt_len))\n",
        "    return population\n",
        "\n",
        "def find_maximal_prompt(pipeline, classifier, candidate_labels, tokenizer, prompt_len: int = 7, population_size: int = 200, num_iterations: int = 150):\n",
        "    vocab = list(tokenizer.vocab.keys())\n",
        "    population = create_initial_population(vocab, prompt_len=prompt_len, population_size=population_size)\n",
        "\n",
        "    for _ in tqdm(range(num_iterations)):\n",
        "        new_population = copy.deepcopy(population)\n",
        "\n",
        "        # create 50 crossovers\n",
        "        for _ in range(population_size):\n",
        "            sample1, sample2 = random.sample(population, 2)\n",
        "            pivot_idx = random.choice(range(prompt_len))\n",
        "            new_population.append(sample1[:pivot_idx] + sample2[pivot_idx:])\n",
        "            new_population.append(sample2[:pivot_idx] + sample1[pivot_idx:])\n",
        "\n",
        "        # create 50 mutations\n",
        "        for _ in range(population_size):\n",
        "            sample = random.choice(population)\n",
        "            num_muts = random.randint(0, prompt_len)\n",
        "            if not num_muts:\n",
        "                continue\n",
        "            mut_idx = random.sample(range(prompt_len), num_muts)\n",
        "            mut_sample = copy.deepcopy(sample)\n",
        "            for i_ in mut_idx:\n",
        "                mut_sample[i_] = random.choice(vocab)\n",
        "            if mut_sample not in new_population:\n",
        "                new_population.append(mut_sample)\n",
        "        population = sorted(\n",
        "            new_population, reverse=True, key=lambda s: fitness(s, candidate_labels, pipeline, tokenizer))[: population_size]\n",
        "\n",
        "    return population[:5]\n",
        "\n",
        "\n",
        "candidate_labels = [\"computational biology\", \"non computational biology\"]\n",
        "find_maximal_prompt(pipeline, tokenizer, candidate_labels)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9m99m7jQJDNn",
        "outputId": "3863f643-d607-4299-f595-989b53b93343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-large-mnli and are newly initialized because the shapes did not match:\n",
            "- classification_head.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "- classification_head.out_proj.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def fitness(sequence, classifier, generator, target_tokens, tokenizer, min_value=-10e2, max_length=300):\n",
        "#   sequences = pipeline(\n",
        "#                           sequence,\n",
        "#                           do_sample=True,\n",
        "#                           top_k=10,\n",
        "#                           num_return_sequences=1,\n",
        "#                           eos_token_id=tokenizer.eos_token_id,\n",
        "#                           max_length=max_length,\n",
        "#                         )\n",
        "#   scores = sequences[0]['generated_sequence']['scores']\n",
        "#   answer = tokenizer.decode(sequences[0][\"generated_sequence\"][\"sequences\"][0])\n",
        "#   best_fit = -np.inf\n",
        "\n",
        "#   for w in range(max_length):\n",
        "#     if w + len(target_tokens) <= len(scores):\n",
        "\n",
        "#       fit = [scores[w+i].squeeze()[token] for i,token in enumerate(target_tokens)]\n",
        "#       fit = list(map(lambda x: min_value if x == -np.inf else x, fit))\n",
        "#       sum_fit = sum(fit)\n",
        "\n",
        "#       if sum_fit > best_fit:\n",
        "#         best_fit = sum_fit\n",
        "#         sent =  [torch.argmax(scores[w+i].squeeze()) for i,token in enumerate(target_tokens)]\n",
        "#         #print(tokenizer.decode(sent))\n",
        "\n",
        "#   return best_fit\n",
        "\n",
        "# for s in sequences:\n",
        "#   results = fitness(s, candidate_labels)\n",
        "#   print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R70PtdoFxWQL",
        "outputId": "1068f642-d263-40ab-88d9-8eb4a5e003dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14973539113998413"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}